# üìö Introduction to Amazon SageMaker

[**‚¨ÖÔ∏è Retornar**](../../index.html)

---

## üì∫ V√≠deo

<div class="video-container" style="text-align: center;">
    <iframe
        style="max-width: 750px; width: 100%; height: 422px;"
        src="https://www.youtube.com/embed/KnNWlfH0TPU?si=JBGNVS4i4GNMwODJ"
        title="Introduction to Amazon SageMaker"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
        referrerpolicy="strict-origin-when-cross-origin"
        allowfullscreen
    ></iframe>
</div>

---

<style>
.columns {
    display: flex;
    gap: 30px;
    align-items: flex-start;
}
.column {
    flex: 1;
    min-width: 300px;
}
p {
    margin-bottom: 1rem;
}
</style>

<div class="columns">

<div class="column">

### English

<p>Arise is an observability and evaluation platform. Um it allows machine learning engineers, AI engineers to build, evaluate and then</p>
<p>to build, evaluate and then productionize their specific ML or AI applications. So Arise is a is a tool that is purchasable through the AWS</p>
<p>that is purchasable through the AWS marketplace. It deploys directly in the the AWS cloud and then is powered by those Nvidia GPUs in order to serve</p>
<p>those Nvidia GPUs in order to serve clients with the agentic evaluation platform. Lots of teams, right, want to dive into the AI world. they want to</p>
<p>dive into the AI world. they want to utilize all of this new LLM technology. AWS with Bedrock makes it super easy to deploy those models into their their</p>
<p>deploy those models into their their different environments. They don't need to, you know, own all of that infrastructure and all of that that</p>
<p>infrastructure and all of that that technology. We are right using Bedrock to deploy various different types of models. So be it GPT40, be it the cloud</p>
<p>models. So be it GPT40, be it the cloud sonnet models, teams can plug into to bedrock via an API or they can access those models and actually interact with</p>
<p>those models and actually interact with those models directly from bedrock. So it's just a few clicks and the models are there. And from a Nvidia</p>
<p>are there. And from a Nvidia perspective, we are working pretty closely with their Nemo microservices team. So we've got what we're</p>
<p>team. So we've got what we're calling the data flywheel where Arise helps teams to collect and analyze how their AI applications are</p>
<p>analyze how their AI applications are performing. Nvidia kind of sits at that at the base level, provides all of that compute power, provides the GPUs and all</p>
<p>compute power, provides the GPUs and all the the technology to actually run the models. Um AWS orchestrating the the model deployments and then Arise is</p>
<p>model deployments and then Arise is going to sit at that top level um allowing you to evaluate and and understand how those models are</p>
<p>understand how those models are performing once they get into either development or the the production world. So we work with a few teams that are</p>
<p>So we work with a few teams that are doing running some fine-tuning pipelines with Nvidia's microservices using you know AWS to kind of store all of</p>
<p>you know AWS to kind of store all of that data. After running through those services, we are able to to cut down latency time. So, lots of teams like to</p>
<p>latency time. So, lots of teams like to work with Nvidia. They give you the microservices ecosystem, which makes it very easy to do fine-tuning and set up</p>
<p>very easy to do fine-tuning and set up guardrails for your specific use cases.</p>

</div>

<div class="column">

### Tradu√ß√£o

<p>Arise √© uma plataforma de observabilidade e avalia√ß√£o.Hum, isso permite que engenheiros de aprendizado de m√°quina e engenheiros de IA construam, avaliem e ent√£o</p>
<p>para construir, avaliar e ent√£o produzir seus aplicativos espec√≠ficos de ML ou IA.So Arise √© uma ferramenta que pode ser adquirida atrav√©s da AWS</p>
<p>que pode ser adquirido por meio do mercado AWS.Ele √© implantado diretamente na nuvem AWS e, em seguida, √© alimentado por essas GPUs Nvidia para atender</p>
<p>essas GPUs Nvidia para atender clientes com a plataforma de avalia√ß√£o de agentes.Muitas equipes querem mergulhar no mundo da IA.eles querem</p>
<p>mergulhe no mundo da IA.eles desejam utilizar toda essa nova tecnologia LLM.AWS com Bedrock torna muito f√°cil implantar esses modelos em seus</p>
<p>implantar esses modelos em seus diferentes ambientes.Voc√™ sabe, eles n√£o precisam possuir toda essa infraestrutura e tudo o que</p>
<p>infraestrutura e toda essa tecnologia.Estamos certos ao usar o Bedrock para implantar v√°rios tipos diferentes de modelos.Assim seja GPT40, seja a nuvem</p>
<p>modelos.Seja no GPT40, seja nos modelos de soneto em nuvem, as equipes podem se conectar √† base por meio de uma API ou podem acessar esses modelos e realmente interagir com eles.</p>
<p>esses modelos e realmente interagir com eles diretamente da base rochosa.Ent√£o s√£o apenas alguns cliques e os modelos est√£o l√°.E de uma Nvidia</p>
<p>est√£o l√°.E do ponto de vista da Nvidia, estamos trabalhando em estreita colabora√ß√£o com a equipe de microsservi√ßos Nemo.Ent√£o n√≥s temos o que somos</p>
<p>equipe.Portanto, temos o que chamamos de volante de dados, onde o Arise ajuda as equipes a coletar e analisar como est√£o seus aplicativos de IA.</p>
<p>analisar o desempenho de seus aplicativos de IA.A Nvidia meio que fica no n√≠vel b√°sico, fornece todo esse poder de computa√ß√£o, fornece as GPUs e todos</p>
<p>poder de computa√ß√£o, fornece as GPUs e toda a tecnologia para realmente executar os modelos.Um AWS orquestrando as implanta√ß√µes do modelo e ent√£o Arise √©</p>
<p>implanta√ß√µes de modelo e ent√£o o Arise ficar√° no n√≠vel superior, permitindo que voc√™ avalie e entenda como esses modelos s√£o</p>
<p>entender o desempenho desses modelos quando entram no mundo do desenvolvimento ou da produ√ß√£o.Ent√£o trabalhamos com algumas equipes que est√£o</p>
<p>Ent√£o, trabalhamos com algumas equipes que est√£o executando alguns pipelines de ajuste fino com microsservi√ßos da Nvidia usando a AWS para armazenar todos os</p>
<p>voc√™ conhece a AWS para armazenar todos esses dados.Depois de executar esses servi√ßos, podemos reduzir o tempo de lat√™ncia.Ent√£o, muitas equipes gostam de</p>
<p>tempo de lat√™ncia.Portanto, muitas equipes gostam de trabalhar com a Nvidia.Eles fornecem o ecossistema de microsservi√ßos, o que facilita muito o ajuste fino e a configura√ß√£o</p>
<p>muito f√°cil de fazer ajustes finos e configurar prote√ß√µes para seus casos de uso espec√≠ficos.</p>

</div>

</div>
